
# ğŸ–¥ï¸ Installation de lâ€™ESXi_1
<br>

## Introduction

Dans ce lab, jâ€™utilise la **virtualisation imbriquÃ©e** (*nested virtualization*) en installant **ESXi_1** sur **VMware Workstation**.

> â„¹ï¸ Je ne dÃ©taille pas ici le processus complet d'installation de l'iso ESXi.  

>ğŸ” Pour connaÃ®tre le dÃ©tail de la configuration que j'ai mis en place pour mon ESXi c'est par ici. â¡ï¸ `architecture.md`

<br>

# âš™ï¸ Configuration Ã  effectuer ESXI_1

Voici les principaux paramÃ¨tres Ã  configurer :

- **Carte rÃ©seau principale** : `NAT`  
- **Adresse IP statique** : `192.168.1.170`
- **Serveur DNS principal** : `192.168.1.1`
- **Serveur DNS secondaire (alternate)** : `1.1.1.1`
- **Nom dâ€™hÃ´te (hostname)** : `ESXi`
- **Clavier** : `French`
- **Nom de domaine complet (FQDN)** : `esxi-1.labo.local`



## ğŸ” VÃ©rification rÃ©seau

Une fois lâ€™installation terminÃ©e, je lance un test rÃ©seau via lâ€™interface ESXi.  
âœ… Tous les tests passent **sauf la rÃ©solution DNS**, ce qui est **normal Ã  ce stade**.



Voici une capture illustrant la configuration rÃ©seau de lâ€™ESXi :

![Configuration RÃ©seau ESXi](./images/configuration_ESXI_1.png)

![Configuration RÃ©seau ESXi](./images/test_reseau_esxi1.png)


Ensuite, je vais me connecte Ã  mon ESXi avec on IP static prÃ©cÃ©demment configurÃ©. 
Je monte le disque en VMFS 6 afin dâ€™avoir une data store local.

<br>

# ğŸŒ Configuration du DNS avec Technitium DNS

## Pourquoi un service DNS ?

Pour fonctionner correctement, **vCenter** a besoin dâ€™un **service DNS** fonctionnel sur le rÃ©seau.  
Il est essentiel que les Ã©quipements puissent **communiquer entre eux** Ã  lâ€™aide :
- dâ€™une **entrÃ©e A** (rÃ©solution de nom â†’ adresse IP)
- dâ€™une **entrÃ©e PTR** (rÃ©solution inverse : IP â†’ nom de domaine)

> ğŸ§  Cette configuration permet aussi de simuler un environnement professionnel rÃ©aliste.



## ğŸ› ï¸ Choix de Technitium DNS

Pour ce lab, jâ€™ai choisi **Technitium DNS** car :
- il est **lÃ©ger**,
- **simple dâ€™utilisation**,
- et trÃ¨s **adaptÃ©** aux environnements de test comme celui-ci.



## ğŸ“ CrÃ©ation de la zone DNS

Jâ€™ai crÃ©Ã© une **zone principale** nommÃ©e `labo.local`.

Dans cette zone, jâ€™ai ajoutÃ© les entrÃ©es suivantes :
- `esxi-1.labo.local`
- `esxi-2.labo.local`
- `vcenter.labo.local`

Chaque entrÃ©e dispose de :
- son **enregistrement A** (nom â†’ IP)
- son **enregistrement PTR** (IP â†’ nom)



## ğŸ–¼ï¸ VÃ©rification avec captures dâ€™Ã©cran

Voici une capture dâ€™Ã©cran montrant que les entrÃ©es **PTR** sont bien configurÃ©es :

![PTR Configuration Technitium DNS](./images/test_connexion_SRV_DNS.png)


>## ğŸ“Œ Remarque
>Cette Ã©tape est **cruciale** pour que le vCenter puisse sâ€™installer correctement, surtout si vous prÃ©voyez dâ€™utiliser des certificats, ou que vous souhaitez un fonctionnement optimal de la rÃ©solution DNS dans votre lab.



# ğŸ§© DÃ©ploiement du vCenter

## 1ï¸âƒ£ Lancement de lâ€™installation

Ã€ ce stade de mon lab, je peux commencer la premiÃ¨re Ã©tape de la configuration du **vCenter**.

Ã‰tant donnÃ© que je suis sur un **home lab** avec des ressources matÃ©rielles limitÃ©es, jâ€™ai choisi un **dÃ©ploiement de taille "TrÃ¨s petite"** (*Tiny Deployment*).

![Choix de la taille - Ã©tape 1](./images/deploiement_vcenter-1.png)  
![Choix de la taille - Ã©tape 2](./images/deploiement_vcenter-2.png)
![Choix de la taille - Ã©tape 3](./images/deploiement_vcenter-3.png)




## 2ï¸âƒ£ Configuration rÃ©seau du vCenter

Voici les paramÃ¨tres rÃ©seau que jâ€™ai configurÃ©s :

- **Adresse IP statique** : `192.168.1.200`
- **Passerelle** : `192.168.1.254`
- **DNS** : `192.168.1.12`  
  > â„¹ï¸ Il sâ€™agit de lâ€™adresse IP de **mon hÃ´te**, qui fait dÃ©sormais office de **serveur DNS grÃ¢ce Ã  Technitium DNS**.
- **Nom dâ€™hÃ´te (FQDN)** : `vcenter.labo.local`

![Choix de la taille - Ã©tape 4](./images/deploiement_vcenter-4.png)



## 3ï¸âƒ£ Fin de la premiÃ¨re Ã©tape

Lâ€™Ã©tape 1 du dÃ©ploiement est terminÃ©e.  
Voici un **rÃ©capitulatif de la configuration** :

![Choix de la taille - Ã©tape 5](./images/deploiement_vcenter-5.png)
![Choix de la taille - Ã©tape 6](./images/deploiement_vcenter-6.png)
![Choix de la taille - Ã©tape 6](./images/etape_1_conf_vcenter_done.png)


## 4ï¸âƒ£ Connexion Ã  l'interface de gestion

Une fois la configuration terminÃ©e, je peux me connecter Ã  **lâ€™interface de gestion du vCenter** via lâ€™URL fournie, par exemple : http://192.168.1.200:5480

![Instance vCenter](./images/etape_2_conf_vcenter_done4.png)



## 5ï¸âƒ£ Configuration de lâ€™instance vCenter Server

Je poursuis avec la **seconde phase** :  
> ğŸ”§ *Configurer une instance de vCenter Server*



## 6ï¸âƒ£ Configuration du domaine SSO (Single Sign-On)

Je configure ensuite le domaine **SSO (Single Sign-On)**, indispensable pour :
- sâ€™authentifier sur le **vCenter**,
- mais aussi sur dâ€™autres produits VMware intÃ©grÃ©s au lab.

Nom du domaine SSO : `vsphere.local`  
Nom dâ€™utilisateur : `administrator@vsphere.local`

![SSO Configuration](./images/etape_2_conf_vcenter_done.png)



## âœ… Connexion finale

Une fois la **deuxiÃ¨me Ã©tape** terminÃ©e, je peux me connecter **directement** via lâ€™URL du vCenter.

![Connexion rÃ©ussie](./images/connexion_vsphere_done.png)



## ğŸ“¦ Ã€ retenir

- Choisir une **taille adaptÃ©e** Ã  vos ressources
- Ne pas oublier de **configurer correctement le DNS** pour Ã©viter les erreurs Ã  lâ€™Ã©tape 2
- Le **domaine SSO** est clÃ© pour l'authentification centralisÃ©e

  



# ğŸ–¥ï¸ Installation de ESXi_2

## ğŸ¯ Objectif

Ã€ cette Ã©tape, je vais installer un **deuxiÃ¨me hÃ´te ESXi** afin de commencer Ã  structurer mon infrastructure autour dâ€™un **Datacenter virtuel** dans vCenter.

> â„¹ï¸ Je ne dÃ©taille pas ici le processus complet d'installation de l'iso ESXi.

>ğŸ” Pour connaÃ®tre le dÃ©tail de la configuration que j'ai mis en place pour mon ESXi c'est par ici. > `architecture.md`



## ğŸ—ï¸ CrÃ©ation du Datacenter

Depuis lâ€™interface de vCenter, je commence par **crÃ©er un Datacenter**, qui servira Ã  regrouper mes hÃ´tes ESXi.

![installation_esxi_2_done](./images/creation_datacenter_done.png)



## â• Ajout de lâ€™hÃ´te ESXi_2

Pour ajouter mon second ESXi Ã  vCenter, je suis les Ã©tapes suivantes :

1. Cliquer sur `Ajouter un hÃ´te`
2. Saisir lâ€™**adresse IP** ou le **FQDN** de lâ€™ESXi (`192.168.1.172` par exemple)
3. Renseigner les identifiants de connexion :
   - **Nom dâ€™utilisateur** : `root`
   - **Mot de passe** : `********`
4. Confirmer les informations
5. Attribuer une **licence** (jâ€™utilise ici la version dâ€™**Ã©valuation**)

![installation_esxi_2_done](./images/config_ipv4_esxi_sous_reseau_.png)


## ğŸ“¦ RÃ©sultat

Une fois ajoutÃ©, les **machines virtuelles dÃ©jÃ  prÃ©sentes sur ESXi_2** apparaissent automatiquement dans lâ€™inventaire vCenter.

> ğŸ”„ Cela facilite la gestion centralisÃ©e de toutes les VM depuis une seule interface.

![Ajout ESXi 2](./images/arborescence_done.png)



## âœ… Prochaine Ã©tape

Maintenant que les deux hÃ´tes ESXi sont intÃ©grÃ©s Ã  vCenter, je vais pouvoir configurer :
- Un **cluster**  
GrÃ¢ce au cluster je vais regrouper mes ESXI et Ã©galement pouvoir utiliser des options comme H.A vMotion, DRS, FT.

- Les **rÃ©seaux distribuÃ©s**  
AmÃ©lirore la gestion et la mobilitÃ© des VM.

- Les **stockages partagÃ©s**    
(pour simuler un environnement de production encore plus rÃ©aliste)

# ğŸ’¾ Installation & Configuration de FreeNAS

## ğŸ¯ Objectif

Pour simuler un environnement professionnel et tirer parti des fonctionnalitÃ©s avancÃ©es de **vCenter**, jâ€™ai besoin dâ€™un **stockage partagÃ©** accessible par mes hÃ´tes ESXi.  
Dans ce lab, jâ€™utilise **FreeNAS** (ancienne version de TrueNAS) pour Ã©muler une **baie SAN (Storage Area Network)**.

> ğŸ’¡ Jâ€™ai optÃ© pour FreeNAS pour sa lÃ©gÃ¨retÃ©, car mes ressources matÃ©rielles sont limitÃ©es.

Une fois la VM dÃ©marrÃ©e, jâ€™attribue une **adresse IP fixe** pour lâ€™accÃ¨s principal : 192.168.180





## ğŸŒ Configuration RÃ©seau

Je crÃ©e deux interfaces rÃ©seau distinctes sur FreeNAS pour sÃ©parer les protocoles de partage :

- **NFS** â†’ `172.16.10.10/24`  
- **iSCSI** â†’ `172.17.10.10/24`



## ğŸ—‚ï¸ CrÃ©ation des volumes & partages

1. **CrÃ©ation du volume** :
   - Nom : `NTFS_PartagÃ©`
   - Disque utilisÃ© : `da1` (400 Go)

2. **Partage NFS** :
   - MappÃ© sur le volume `NTFS_PartagÃ©`
   - AccÃ¨s restreint uniquement aux hÃ´tes connectÃ©s au sous-rÃ©seau `172.16.10.0/24` (via une rÃ¨gle de sÃ©curitÃ©)


![FreeNAS Configuration](./images/droit_reseau_nfs.png)


## âœ… Prochaine Ã©tape

Ce volume NFS sera ensuite **montÃ© dans vCenter** pour Ãªtre utilisÃ© comme **datastore partagÃ©** par mes hÃ´tes ESXi.

Cela me permettra de :
- Migrer des VM entre hÃ´tes (vMotion)
- DÃ©ployer des clusters HA/DRS
- Simuler un environnement professionnel complet  
<br>

# ğŸŒ CrÃ©ation & Configuration des RÃ©seaux de Stockage

## ğŸ¯ Objectif

Lâ€™Ã©tape suivante consiste Ã  **crÃ©er deux rÃ©seaux de stockage** dans VMware Workstation, puis Ã  les connecter Ã  **FreeNAS** et aux deux **hÃ´tes ESXi**.  
Cela permettra Ã  tous les Ã©quipements de communiquer correctement, notamment pour le partage NFS et lâ€™accÃ¨s iSCSI.


## ğŸ› ï¸ Ã‰tapes de configuration

### 1. CrÃ©ation des rÃ©seaux sur VMware Workstation

Dans VMware Workstation :

- CrÃ©er deux rÃ©seaux internes personnalisÃ©s (via lâ€™Ã©diteur de rÃ©seau virtuel) :
  - `VMnet2` pour le **rÃ©seau NFS** (ex. : `172.16.10.0/24`)
  - `VMnet3` pour le **rÃ©seau iSCSI** (ex. : `172.17.10.0/24`)

> ğŸ“ Ces rÃ©seaux ne doivent pas avoir de passerelle par dÃ©faut ni de serveur DHCP : ils sont isolÃ©s pour le stockage.

![FreeNAS Configuration](./images/conf_reseau_vmwareworkstation.png)



### 2. Attribution des interfaces rÃ©seau

Dans la configuration de la VM **FreeNAS**, associer :
- `VMnet2` Ã  lâ€™interface NFS (`172.16.10.10`)
- `VMnet3` Ã  lâ€™interface iSCSI (`172.17.10.10`)

Sur les ESXi, on va crÃ©er un vSwitch dÃ©diÃ© par rÃ©seau, puis y associer une interface **vmkernel**.



### 3. Monter le stockage NFS

Dans le vCenter, il va falloir monter notre espace de stockage NFS. Il sera crÃ©e sous forme d'un `Datastrore`.

> âš ï¸ Attention : Ne pas se tromper au niveau du chemin au FreeNAS, et indiquer le bon rÃ©seau. 

![FreeNAS Configuration](./images/montage_nfs_ok.png)



## ğŸ”Œ ParamÃ©trage sur ESXi_1

1. AccÃ©der Ã  **RÃ©seau > Commutateurs virtuels**
2. **Ajouter une mise en rÃ©seau**
   - Type : `VMkernel`
   - Nouveau vSwitch : `vSwitch-NFS`
   - Carte rÃ©seau : `vmnic1` (connectÃ©e Ã  `VMnet2`)
   - IPv4 statique : `172.16.10.11`
   - Nom du groupe de ports : `NFS-Net`

![FreeNAS Configuration](./images/montage_nfs_esxi1_2.png)



## ğŸ” RÃ©pÃ©ter sur ESXi_2

MÃªme procÃ©dure, en adaptant :

- IP statique : `172.16.10.12`
- **Nom du port group** : **identique** Ã  celui dâ€™ESXi_1 (`NFS-Net`) pour assurer la compatibilitÃ©

> âš ï¸ Attention : les noms des port groups doivent strictement Ãªtre identiques pour Ã©viter des soucis lors du montage NFS dans vCenter.

![FreeNAS Configuration](./images/vswitch_esxi1.png)


## âœ… RÃ©sultat attendu

Ã€ lâ€™issue de cette configuration :

- Le **SAN (FreeNAS)** pourra dialoguer avec les deux **ESXi** via le rÃ©seau NFS
- La connectivitÃ© rÃ©seau dÃ©diÃ©e au stockage est **isolÃ©e du rÃ©seau de gestion**
- Le montage du **datastore NFS** sera possible dans vCenter

![FreeNAS Configuration](./images/connexion_esxi_SAN_NFS.png)
<br>
<br>

# ğŸ’¾ Montage dâ€™un Datastore iSCSI (LUN)

## ğŸ¯ Objectif

AprÃ¨s avoir configurÃ© le rÃ©seau de stockage iSCSI, je vais maintenant **monter un datastore iSCSI** sur mes hÃ´tes ESXi Ã  lâ€™aide de **FreeNAS**.  
Lâ€™iSCSI est plus complexe que NFS, mais Ã©galement plus rÃ©pandu dans les environnements professionnels.



## ğŸ› ï¸ Configuration iSCSI sur FreeNAS

### ğŸ“Œ Ã‰tapes dans lâ€™interface FreeNAS

1. **Bloc > Portails**
   - Ajouter un nouveau portail
   - Interface : `172.17.10.10`
   - Nom : `Portail iSCSI`

2. **Initiateurs**
   - Ajouter un initiateur
   - Initiateurs : `ALL`
   - RÃ©seau autorisÃ© : `172.17.10.0/24` (Plus sÃ©curisÃ©)

3. **Cibles**
   - Ajouter une cible nommÃ©e `Target_FreeNAS`
   - Associer au portail et Ã  lâ€™initiateur crÃ©Ã©s

4. **Extensions**
   - Ajouter une extension iSCSI Ã  partir du disque `da2` (ex. : `Disque_3`)

5. **Cibles > Extensions**
   - Associer la cible `Target_FreeNAS` au disque `Disque_3` avec le **LUN ID = 1**

6. **Services**
   - Activer le service **iSCSI**



## âš™ï¸ Configuration rÃ©seau iSCSI sur les hÃ´tes ESXi

### ğŸ”§ Sur ESXi_1

1. AccÃ©der Ã  **RÃ©seau > Commutateurs virtuels**
2. Ajouter une mise en rÃ©seau :
   - Type : `VMkernel`
   - Nouveau vSwitch : `vSwitch-iSCSI`
   - Carte rÃ©seau : `vmnic2`
   - IPv4 statique : `172.17.10.11`
   - Nom du groupe de ports : `iSCSI-Net`

### ğŸ” RÃ©pÃ©ter la mÃªme configuration sur ESXi_2

- IP statique : `172.17.10.12`
- Utiliser le mÃªme nom de groupe de ports (`iSCSI-Net`)



## ğŸ”Œ Connexion entre FreeNAS et les ESXi

Une fois le rÃ©seau fonctionnel, je vais lier lâ€™adaptateur iSCSI logiciel aux groupes de ports vmkernel.

### ğŸ“ Sur ESXi_1

1. **Configurer > Adaptateurs de stockage**
2. SÃ©lectionner lâ€™adaptateur `vmhba65` (iSCSI logiciel)
3. **Liaison de port rÃ©seau > Ajouter**
   - SÃ©lectionner le port vmkernel : `iSCSI-Net`

4. **DÃ©couverte dynamique > Ajouter serveur de cible**
   - IP cible : `172.17.10.10`

### ğŸ” RÃ©pÃ©ter sur ESXi_2 avec les mÃªmes paramÃ¨tres



## ğŸ Montage du Datastore iSCSI

1. Dans **vCenter > Datacenter**, clic droit :
   - `Stockage > Nouvelle banque de donnÃ©es`
2. Choisir :
   - Type : `VMFS`
   - Nom : `iSCSI_FreeNAS`
   - SÃ©lectionner le **LUN nÂ°1**
   - Format : `VMFS 6`

ğŸ’¡ Comme mes deux ESXi sont dans le **mÃªme Datacenter**, le second ESXi accÃ¨de automatiquement au **datastore partagÃ©**.

![FreeNAS Configuration](./images/montage_iSCSI_test_ecriture_folder.png)


## âœ… RÃ©sultat attendu

- Un **datastore iSCSI** fonctionnel, visible sur les deux ESXi
- Une infrastructure **proche dâ€™un environnement professionnel**
- PossibilitÃ© de crÃ©er des **VMs partagÃ©es** entre les deux hÃ´tes

![test_iscsi](./images/connextion_iscsi_esxi_test_ssh.png)

<br>
<br>

# ğŸš€ Profiter des fonctionnalitÃ©s vSphere

Maintenant que mon infrastructure est fonctionnelle, je vais pouvoir exploiter les puissantes options offertes par la suite **vSphere**, notamment :

- **H.A. (High Availability)**
- **vMotion**
- **DRS (Distributed Resource Scheduler)**

Ces outils permettent d'optimiser la disponibilitÃ©, la mobilitÃ© des machines virtuelles et la gestion des ressources dans un environnement virtualisÃ©.

Pour plus d'informations dÃ©taillÃ©es sur ces options, je vous invite Ã  consulter le fichier [**readme.md**](./readme.md).
